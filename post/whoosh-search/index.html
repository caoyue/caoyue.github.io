<!doctype html><html lang=zh-Hans><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="default"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#a6d6d6"><meta name=referrer content="no-referrer"><meta name=description content="Whoosh 是一个纯 Python 实现的全文搜索组件。基础架构和 Lucene 比较像。使用试了试，记录一些东西。
中文分词
Whoosh 本身只有英文分词，因此需要添加中文分词组件。
最后选择了 Jieba 这个 Python 中文分词组件，初步测试分词效果还不错。有时间会把几个中文分词组件对比一下看看。
Jieba 已经封装好了 ChineseAnalyzer for Whoosh，只需要引用 from jieba.analyse import ChineseAnalyzer 来替换 Whoosh 的 Analyzer 即可。
HTML 抽取
对于纯文本直接分析建立索引即可。
而对于 HTML 文本，我们需要先将其中的文本抽取出来再进行运行分析程序。否则其中的 HTML 标签将会被当作文本来分析，比如搜索 &ldquo;span&rdquo; 将会得到所有包含 <span></span> 的内容。举个例子，用 HTMLParser 来提取文本，其他类似功能的模块也有不少。
def html_strip(html): from HTMLParser import HTMLParser html = html.strip() html = html.strip(&#34;\n&#34;) result = [] parse = HTMLParser() parse.handle_data = result.append parse.feed(html) parse.close() return &#34;&#34;.join(result) 关键词 Highlight
默认的高亮结果只会包含结果命中的部分碎片，需要不同展示可以使用不同的 Fragmenters 。比如展示全文需要 whoosh."><meta name=author content="caoyue"><title>且听疯吟 / Whoosh 全文搜索</title><link rel=alternate type=application/rss+xml href=/index.xml title=且听疯吟><link rel=apple-touch-icon sizes=192x192 href=https://blog.caoyue.me/favicon.png><link rel=icon type=image/png sizes=48x48 href=https://blog.caoyue.me/favicon.png><link rel=icon type=image/png sizes=192x192 href=https://blog.caoyue.me/favicon.png><link rel=stylesheet type=text/css href=https://blog.caoyue.me/normalize.css><link rel=stylesheet type=text/css href=https://blog.caoyue.me/scss/style.css><link rel=stylesheet type=text/css href=https://blog.caoyue.me/syntax/monokailight.css id=syntax-theme><script type=text/javascript>function syntaxHighlight(){var e=document.querySelector("#syntax-theme");let t="https://blog.caoyue.me/syntax/monokailight.css",n="https://blog.caoyue.me/syntax/dracula.css",s=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?n:t;e.href=s}syntaxHighlight(),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",()=>{syntaxHighlight()})</script><script src=https://kit.fontawesome.com/061cfdc036.js crossorigin=anonymous></script></head><body><div class=wrap><header class=header><a href=/ title=且听疯吟>且听疯吟</a>
<span>如此生活三十年</span></header><div class=left><ul class=list><li><a href=/ title=Home>Home</a></li><li><a href=/tweets title=碎碎念>Tweets</a></li><li><a href=/archives title=Archives>Archives</a></li><li><a href=/tags title=Tags>Tags</a></li></ul></div><div class=right><div class=entry id=cotent><div class=title>Whoosh 全文搜索</div><div class=content><p>Whoosh 是一个纯 Python 实现的全文搜索组件。基础架构和 Lucene 比较像。使用试了试，记录一些东西。</p><p><strong>中文分词</strong><br>Whoosh 本身只有英文分词，因此需要添加中文分词组件。<br>最后选择了 <a href=https://github.com/fxsjy/jieba>Jieba</a> 这个 Python 中文分词组件，初步测试分词效果还不错。有时间会把几个中文分词组件对比一下看看。<br>Jieba 已经封装好了 ChineseAnalyzer for Whoosh，只需要引用 <code>from jieba.analyse import ChineseAnalyzer</code> 来替换 Whoosh 的 Analyzer 即可。</p><p><strong>HTML 抽取</strong><br>对于纯文本直接分析建立索引即可。<br>而对于 HTML 文本，我们需要先将其中的文本抽取出来再进行运行分析程序。否则其中的 HTML 标签将会被当作文本来分析，比如搜索 &ldquo;span&rdquo; 将会得到所有包含 <code>&lt;span>&lt;/span></code> 的内容。举个例子，用 HTMLParser 来提取文本，其他类似功能的模块也有不少。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=k>def</span> <span class=nf>html_strip</span><span class=p>(</span><span class=n>html</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>HTMLParser</span> <span class=kn>import</span> <span class=n>HTMLParser</span>
</span></span><span class=line><span class=cl>    <span class=n>html</span> <span class=o>=</span> <span class=n>html</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>html</span> <span class=o>=</span> <span class=n>html</span><span class=o>.</span><span class=n>strip</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>parse</span> <span class=o>=</span> <span class=n>HTMLParser</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>parse</span><span class=o>.</span><span class=n>handle_data</span> <span class=o>=</span> <span class=n>result</span><span class=o>.</span><span class=n>append</span>
</span></span><span class=line><span class=cl>    <span class=n>parse</span><span class=o>.</span><span class=n>feed</span><span class=p>(</span><span class=n>html</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parse</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>关键词 Highlight</strong><br>默认的高亮结果只会包含结果命中的部分碎片，需要不同展示可以使用不同的 Fragmenters 。比如展示全文需要 <code>whoosh.highlight.WholeFragmenter</code> 。<br>然而 HTML 的高亮有一个问题。简单的基于匹配的替换带来的问题就是 HTML 标签的属性内容也被替换了，比如 <code>a</code> 标签的 <code>href</code> 属性，导致结构发生错乱。对此除了自己写 HTMLFragmenter 之外似乎没有现成的解决办法。<br>考虑到服务端解析的效率问题，放弃 Whoosh 和服务端的高亮，使用 js 在客户端高亮(其原理也是通过判断关键词前后的标签匹配，并经过一系列的正则替换最终实现只替换文本关键词而忽略标签)。试过效果比较好的高亮方案，<a href=https://github.com/jbr/jQuery.highlightRegex>https://github.com/jbr/jQuery.highlightRegex</a><br>只需要在 <code>results = searcher.search(q, terms=True)</code> 时设置 <code>terms=True</code> 即可从 results 或 results 的 hit 中取得关键词 <code>terms = results.matched_terms()</code>，然后将关键词传递给前端用 highlightRegex 来高亮。</p><p><strong>结果分页</strong><br>对于结果的分页，whoosh 提供了 <code>search_page</code> 方法。但是这个方法可以说是个半成品。首先，search_page 方法支持的参数设置较少，很多功能没法在 search_page 中完成。其次，search_page 方法返回的结果为 ResultsPage 类型，而 <code>search</code> 方法返回 Results 类型，且这两者之前并无继承关系，Results 中包含的属性比 ResultsPage 丰富得多。<br>最重要的是，到目前为止，使用 search_page 方法从所有结果中获取中间页时，其性能与使用 search 获取所有结果然后手动分页是一样的，从源代码可以看到 search_page 仅仅是对 search 的一次封装。search_page 仅仅是出于方便使用的功能（虽然我也没看出 search_page 存在的意义和方便在哪…… ）<br>因此，还是使用 search 的 limit 参数来满足分页需求。limit 参数限制了返回的结果数目。可以使用</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>searcher</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>limit</span><span class=o>=</span><span class=n>page</span> <span class=o>*</span> <span class=n>pagesize</span><span class=p>)</span>
</span></span></code></pre></div><p>来控制返回的结果，然后使用</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=n>results</span><span class=p>[(</span><span class=n>page</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>pagesize</span><span class=p>:</span><span class=n>page</span> <span class=o>*</span> <span class=n>pagesize</span><span class=p>]</span>
</span></span></code></pre></div><p>获取指定的分页。</p><p><strong>词典选择</strong><br>中文分词的效果有很大一部分取决于词典，但并不是词典越大越全越好。分析词典 Build Trie 是一个比较消耗 CPU 的过程（虽然只是在第一次需要进行这个过程，之后会读取 Cache 中的 Model），越大的词典分析时消耗的资源也越大。因此根据实际情况选择词典比较好。<br>此外，如果需要分析的文本包含许多专业性词汇，也可以考虑设置自定义词典来增强歧义分析能力。<br>词典的设置很简单，使用 <code>jieba.set_dictionary(dict_path)</code> 即可。</p><p><strong>其他</strong></p><p>虽然 Whoosh 的性能不尽如人意，相关资料和扩展也缺乏。<br>但总体来说，对于小规模的使用，whoosh 开发简单，基本可以满足需求，如果使用过 Lucene 也可以很容易上手。而且纯 Python 实现，看源代码也方便。</p></div><div class=tags><ul class=info><li>2013-08-05</li><li><a href=https://blog.caoyue.me/tags/python>#python</a></li><li class=button title="show comments" id=toggle-comments><i class="fa-solid fa-message"></i></li></ul></div></div><div class=comments id=comments><section class="article discussion" style=display:none><script>function loadComment(){let t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"photon-dark":"github-light",e=document.createElement("script");e.src="https://utteranc.es/client.js",e.setAttribute("repo","caoyue/caoyue.github.io"),e.setAttribute("issue-term","pathname"),e.setAttribute("theme",t),e.setAttribute("label","Comment"),e.setAttribute("crossorigin","anonymous"),e.setAttribute("async",""),document.querySelector("section.article.discussion").innerHTML="",document.querySelector("section.article.discussion").appendChild(e)}loadComment(),window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",()=>{loadComment()});function toggleComment(){let e=document.querySelector("section.article.discussion");e.style.display=="none"?e.style.display="block":e.style.display="none"}document.querySelector("#toggle-comments").addEventListener("click",toggleComment)</script></section></div></div><footer class=footer><div>©2023 <a href=/>且听疯吟</a>. designed
by <a href=https://caoyue.me target=_blank>caoyue</a>.
powered by <a href=https://gohugo.io/ target=_blank>hugo</a>.
<script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "1a24e73fd4284bfa8b43ecf5a5c2c100"}'></script>
<script defer src=https://use.typekit.net/jdb5cpn.js></script>
<script type=text/javascript>try{Typekit.load({async:!0})}catch{}</script></div></footer></div></body></html>