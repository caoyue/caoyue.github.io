<html lang="zh"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="default"/><meta name="format-detection" content="telephone=no"/><meta name="theme-color" content="#a6d6d6"/><meta name="msapplication-TileColor" content="#a6d6d6"/><meta name="msapplication-TileImage" content="/icon/favicon.ico"/><meta name="referrer" content="no-referrer"/><title>且听疯吟 / Whoosh 全文搜索</title><link rel="alternate" type="application/rss+xml" title="atom 1.0" href="/atom.xml"/><link rel="apple-touch-icon" sizes="192x192" href="/icon/favicon.ico"/><link rel="icon" type="image/png" sizes="48x48" href="/icon/favicon.ico"/><link rel="icon" type="image/png" sizes="192x192" href="/icon/favicon.ico"/><link rel="stylesheet" href="/css/normalize.css"/><link rel="stylesheet" href="/css/style.css"/><meta name="generator" content="Hexo 6.0.0"></head><body><div class="wrap"><div class="header"><a href="/" title="this is a link">且听疯吟</a><span>如此生活三十年</span></div><div class="left"><ul class="list"><li><a href="/" title="Home">Home</a></li><li><a href="/archives" title="archive">Archive</a></li><li><a href="/tags" title="Tags">Tags</a></li></ul></div><div class="right"><div class="entry" id="content"><div class="title">Whoosh 全文搜索</div><div class="content"><p>Whoosh 是一个纯 Python 实现的全文搜索组件。基础架构和 Lucene 比较像。使用试了试，记录一些东西。</p>
<p><strong>中文分词</strong><br>Whoosh 本身只有英文分词，因此需要添加中文分词组件。<br>最后选择了 <a target="_blank" rel="noopener" href="https://github.com/fxsjy/jieba">Jieba</a> 这个 Python 中文分词组件，初步测试分词效果还不错。有时间会把几个中文分词组件对比一下看看。<br>Jieba 已经封装好了 ChineseAnalyzer for Whoosh，只需要引用 <code>from jieba.analyse import ChineseAnalyzer</code> 来替换 Whoosh 的 Analyzer 即可。</p>
<p><strong>HTML 抽取</strong><br>对于纯文本直接分析建立索引即可。<br>而对于 HTML 文本，我们需要先将其中的文本抽取出来再进行运行分析程序。否则其中的 HTML 标签将会被当作文本来分析，比如搜索 “span” 将会得到所有包含 <code>&lt;span&gt;&lt;/span&gt;</code> 的内容。举个例子，用 HTMLParser 来提取文本，其他类似功能的模块也有不少。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_strip</span>(<span class="params">html</span>):</span></span><br><span class="line">    <span class="keyword">from</span> HTMLParser <span class="keyword">import</span> HTMLParser</span><br><span class="line">    html = html.strip()</span><br><span class="line">    html = html.strip(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    result = []</span><br><span class="line">    parse = HTMLParser()</span><br><span class="line">    parse.handle_data = result.append</span><br><span class="line">    parse.feed(html)</span><br><span class="line">    parse.close()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(result)</span><br></pre></td></tr></table></figure>

<p><strong>关键词 Highlight</strong><br>默认的高亮结果只会包含结果命中的部分碎片，需要不同展示可以使用不同的 Fragmenters 。比如展示全文需要 <code>whoosh.highlight.WholeFragmenter</code> 。<br>然而 HTML 的高亮有一个问题。简单的基于匹配的替换带来的问题就是 HTML 标签的属性内容也被替换了，比如 <code>a</code> 标签的 <code>href</code> 属性，导致结构发生错乱。对此除了自己写 HTMLFragmenter 之外似乎没有现成的解决办法。<br>考虑到服务端解析的效率问题，放弃 Whoosh 和服务端的高亮，使用 js 在客户端高亮(其原理也是通过判断关键词前后的标签匹配，并经过一系列的正则替换最终实现只替换文本关键词而忽略标签)。试过效果比较好的高亮方案，<a target="_blank" rel="noopener" href="https://github.com/jbr/jQuery.highlightRegex">https://github.com/jbr/jQuery.highlightRegex</a><br>只需要在 <code>results = searcher.search(q, terms=True)</code> 时设置 <code>terms=True</code> 即可从 results 或 results 的 hit 中取得关键词 <code>terms = results.matched_terms()</code>，然后将关键词传递给前端用 highlightRegex 来高亮。</p>
<p><strong>结果分页</strong><br>对于结果的分页，whoosh 提供了 <code>search_page</code> 方法。但是这个方法可以说是个半成品。首先，search_page 方法支持的参数设置较少，很多功能没法在 search_page 中完成。其次，search_page 方法返回的结果为 ResultsPage 类型，而 <code>search</code> 方法返回 Results 类型，且这两者之前并无继承关系，Results 中包含的属性比 ResultsPage 丰富得多。<br>最重要的是，到目前为止，使用 search_page 方法从所有结果中获取中间页时，其性能与使用 search 获取所有结果然后手动分页是一样的，从源代码可以看到 search_page 仅仅是对 search 的一次封装。search_page 仅仅是出于方便使用的功能（虽然我也没看出 search_page 存在的意义和方便在哪…… ）<br>因此，还是使用 search 的 limit 参数来满足分页需求。limit 参数限制了返回的结果数目。可以使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">results = searcher.search(q, limit=page * pagesize)</span><br></pre></td></tr></table></figure>

<p>来控制返回的结果，然后使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">results[(page - <span class="number">1</span>) * pagesize:page * pagesize]</span><br></pre></td></tr></table></figure>

<p>获取指定的分页。</p>
<p><strong>词典选择</strong><br>中文分词的效果有很大一部分取决于词典，但并不是词典越大越全越好。分析词典 Build Trie 是一个比较消耗 CPU 的过程（虽然只是在第一次需要进行这个过程，之后会读取 Cache 中的 Model），越大的词典分析时消耗的资源也越大。因此根据实际情况选择词典比较好。<br>此外，如果需要分析的文本包含许多专业性词汇，也可以考虑设置自定义词典来增强歧义分析能力。<br>词典的设置很简单，使用 <code>jieba.set_dictionary(dict_path)</code> 即可。</p>
<p><strong>其他</strong></p>
<p>虽然 Whoosh 的性能不尽如人意，相关资料和扩展也缺乏。<br>但总体来说，对于小规模的使用，whoosh 开发简单，基本可以满足需求，如果使用过 Lucene 也可以很容易上手。而且纯 Python 实现，看源代码也方便。</p>
</div><div><ul class="info"><li>posted on 2013-08-05<li><a href="/tags/python/">#python</a></li></li></ul></div></div><div id="disqus_thread"></div><script>var disqus_config = function () {
    this.page.url = "https://blog.caoyue.me/post/whoosh-search/";
    this.page.identifier = "post/whoosh-search/";
};

(function() {
    var d = document, s = d.createElement('script');
    s.src = 'https://caoyue.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();</script></div><div class="footer"><div></div>©2021<a href="https://blog.caoyue.me"> 且听疯吟</a>. designed by<a href="https://caoyue.me" target="_blank"> caoyue</a>. powered by<a href="https://hexo.io/" target="_blank"> hexo</a></div><script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon='{"token": "1a24e73fd4284bfa8b43ecf5a5c2c100"}'></script>
</div></body></html>